About
====
Collaborative robots are a form of robotic automation built to work safely alongside human workers in a shared, collaborative workspace. In most applications, a collaborative robot is responsible for repetitive, menial tasks while a human worker completes more complex and thought-intensive tasks. The accuracy, uptime and repeatability of collaborative robots is designed to complement the intelligence and problem-solving skills of a human worker.

Collaborative robot designs differ greatly from their industrial robot counterparts. Featuring rounded edges, force limitations, and light weights, collaborative robots are first and foremost designed for safety. Most collaborative robots are equipped with a series of sensors to avoid collisions with human workers, as well as safety protocols to shut down if any form of unplanned contact occurs.

The ability to work collaboratively with humans greatly expands the potential applications of robotic automation. The market for collaborative robots is expected to experience exponential growth as more and more industries realize the profits to be gained from this technology.

With major market potential in a number of industries inside and outside of the factory setting, collaborative robots will be an exciting technology to keep an eye on as it becomes a prominent form of robotic automation.

With the above points in mind we set out to make a unique project that combines machine learning and AI with a moveable robotic platform such as a Turtlebot 4 running ROS2 Galactic to get it to follow commands. The commands are hand gestures given by the user remotely using a webcam attached to a laptop. 

A RandomForrestCLassifier has been trained on X and Y coordinates of the hand landmarks generated by the mediapipe python library to accurately predict the commands based on the hand gestures provided by the user. The predicted commands are published as Twist messages over the ROS network to the /cmd_vel topic in the Turtlebot 4.

To learn more or try our project out yourself please visit: https://github.com/VishalNadig/hand-gesture-controlled-turtlebot4

Project Goals
----
Our primary objective is to create a collaborative robot using the Turtlebot 4 and remotely control it in real-time. To achieve this, we plan to utilize American Sign Language (ASL), a widely recognized method of communication using hand gestures. Our approach involves training a machine learning model on a large dataset of ASL images to accurately recognize signs made by the user and control the Turtlebot 4 accordingly. Additionally, we intend to incorporate a learning/teaching mode for the Turtlebot, enabling it to understand new gestures for new commands in the future. After testing the accuracy of the machine learning model, we aim to deploy it on a device connected to the same ROS network as the Turtlebot 4.


Project Workflow
----
Our primary goal for the project was to develop an accurate, efficient machine learning model to recognize American Sign Language (ASL) hand gestures and control the Turtlebot 4 remotely. Initially, we tried using a Convolutional Neural Network (CNN) model with a large dataset of ASL images, but the model was inaccurate, hyper-parameter tuning took a long time, and the system was lagging. We explored various CNN model architectures but found none to be efficient and accurate for our use case. We then chose to use the Mediapipe library for Python, which had a systematic way of collecting hand gesture data using 21 unique landmarks assigned to each part of the human hand. The X and Y coordinates of these landmarks could be exported to a CSV file and used to train a machine learning model.

Due to the inefficiency of the CNN model, we switched to a Random Forest Classifier model from scikit-learn. We also changed the dataset we used for training and testing, as the previous dataset was too large and complicated. We found a smaller dataset on kaggle that contained images of just the numbers used in ASL, which performed better and yielded over 95% accuracy on the model we trained. The Random Forest Classifier model was also much more efficient on the ROS network and used less computational power to predict the hand gestures. 

ROS Architecture
----

Tradeoffs
----

Demo Video
----

Elevator Pitch
----

